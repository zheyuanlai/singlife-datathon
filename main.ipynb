{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Agent-Client Matching via Clustering, Classification, and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and load the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pathlib import Path\n",
    "import prince\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "base_dir = Path(__file__).parents[2]\n",
    "df = pd.read_csv(base_dir / './2025-NUS-datathon/data/merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "# 0:Preprocessing\n",
    "########################################################################\n",
    "\n",
    "# 0.1: Drop blank and NaN\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "# 0.2: Encode agent expertise\n",
    "expertise_list = ['prod_0', 'prod_2', 'prod_4', 'prod_6', 'prod_7', 'prod_8', 'prod_9']\n",
    "for expertise in expertise_list:\n",
    "    df['agent_expertise_' + expertise] = df['agent_product_expertise'].apply(lambda x: 1 if expertise in x else 0)\n",
    "df.drop(columns=['agent_product_expertise'], inplace=True)\n",
    "\n",
    "\n",
    "## 1: Separate dataframes\n",
    "################################################################################################################################################\n",
    "\n",
    "#Select GOOD Rows eg. in force and not expired\n",
    "assessment_columns=['annual_premium','flg_main', 'flg_rider', 'flg_inforce', 'flg_lapsed',\n",
    "       'flg_cancel', 'flg_expire', 'flg_converted']\n",
    "\n",
    "print(df.shape)\n",
    "df=df[df['flg_expire']!=1]\n",
    "print(df.shape)\n",
    "df=df[df['flg_lapsed']!=1]\n",
    "print(df.shape)\n",
    "df=df[df['annual_premium']>0]\n",
    "print(df.shape)\n",
    "\n",
    "client_interest_columns=['annual_premium',\n",
    "       'product',  'product_grp',\n",
    "        'agent_age',\n",
    "       'agent_gender', 'agent_marital', 'agent_tenure', 'cnt_converted',\n",
    "       'annual_premium_cnvrt', 'pct_lapsed', 'pct_cancel', 'pct_inforce',\n",
    "\n",
    "       'pct_prod_0_cnvrt', 'pct_prod_1_cnvrt', 'pct_prod_2_cnvrt',\n",
    "       'pct_prod_3_cnvrt', 'pct_prod_4_cnvrt', 'pct_prod_5_cnvrt',\n",
    "       'pct_prod_6_cnvrt', 'pct_prod_7_cnvrt', 'pct_prod_8_cnvrt',\n",
    "       'pct_prod_9_cnvrt', \n",
    "\n",
    "       'pct_SX0_unknown', 'pct_SX1_male', 'pct_SX2_female',\n",
    "       'pct_AG01_lt20', 'pct_AG02_20to24', 'pct_AG03_25to29',\n",
    "       'pct_AG04_30to34', 'pct_AG05_35to39', 'pct_AG06_40to44',\n",
    "       'pct_AG07_45to49', 'pct_AG08_50to54', 'pct_AG09_55to59',\n",
    "       'pct_AG10_60up', \n",
    "       'cluster',\n",
    "\n",
    "       'agent_expertise_prod_0', 'agent_expertise_prod_2',\n",
    "       'agent_expertise_prod_4', 'agent_expertise_prod_6',\n",
    "       'agent_expertise_prod_7', 'agent_expertise_prod_8',\n",
    "       'agent_expertise_prod_9']\n",
    "\n",
    "agent_interest_columns=['cust_age_at_purchase_grp', 'cust_tenure_at_purchase_grp','cltsex',\n",
    "                         'marryd', 'race_desc_map', 'cltpcode', 'household_size',\n",
    "                        'economic_status', 'family_size', 'household_size_grp',\n",
    "                        'family_size_grp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unsupervised Clustering with GMM\n",
    "\n",
    "## 2: Cluster\n",
    "################################################################################################################################################\n",
    "\n",
    "# 2.1: Use PCA MCA before clustering\n",
    "########################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from sklearn import datasets\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from prince import *\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#encode client data\n",
    "##########################################\n",
    "d=df[client_interest_columns]\n",
    "\n",
    "#find df for mca\n",
    "clustering_categorical_columns = d.select_dtypes(include=['object']).columns\n",
    "\n",
    "clustering_encoded_df=pd.get_dummies(d, columns=clustering_categorical_columns)\n",
    "clustering_encoded_df=clustering_encoded_df.astype(float)\n",
    "# print(clustering_encoded_df.columns)\n",
    "\n",
    "#drop numerical columns\n",
    "clustering_numerical_columns = ['annual_premium', 'agent_age', 'agent_tenure', 'cnt_converted',\n",
    "       'annual_premium_cnvrt', 'pct_lapsed', 'pct_cancel', 'pct_inforce',\n",
    "       'pct_prod_0_cnvrt', 'pct_prod_1_cnvrt', 'pct_prod_2_cnvrt',\n",
    "       'pct_prod_3_cnvrt', 'pct_prod_4_cnvrt', 'pct_prod_5_cnvrt',\n",
    "       'pct_prod_6_cnvrt', 'pct_prod_7_cnvrt', 'pct_prod_8_cnvrt',\n",
    "       'pct_prod_9_cnvrt', 'pct_SX0_unknown', 'pct_SX1_male', 'pct_SX2_female',\n",
    "       'pct_AG01_lt20', 'pct_AG02_20to24', 'pct_AG03_25to29',\n",
    "       'pct_AG04_30to34', 'pct_AG05_35to39', 'pct_AG06_40to44',\n",
    "       'pct_AG07_45to49', 'pct_AG08_50to54', 'pct_AG09_55to59',\n",
    "       'pct_AG10_60up', 'cluster']\n",
    "\n",
    "print('/////////////////////////////')\n",
    "print(clustering_encoded_df.shape)\n",
    "clustering_encoded_df=clustering_encoded_df.drop(columns= clustering_numerical_columns)\n",
    "print(clustering_encoded_df.shape)\n",
    "\n",
    "#The following dataframe is used for mca\n",
    "print(clustering_encoded_df.head())\n",
    "print(clustering_encoded_df.columns)\n",
    "\n",
    "print('checkpoint 1')\n",
    "\n",
    "#perform mca on categorical data\n",
    "mca1 = prince.MCA(n_components=10, copy=True, check_input=True, engine='sklearn', random_state=42)\n",
    "mca1 = mca1.fit(clustering_encoded_df)\n",
    "clustering_df_mca = mca1.transform(clustering_encoded_df)\n",
    "print(clustering_df_mca.head())\n",
    "\n",
    "\n",
    "\n",
    "clustering_df_numeric=d[clustering_numerical_columns]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing code\n",
    "\n",
    "print(clustering_df_numeric.head())\n",
    "#rejoin mca and numerical for random forest predictions\n",
    "\n",
    "gmm_df=pd.concat([clustering_df_mca,clustering_df_numeric],axis=1)\n",
    "\n",
    "#convert column names to string\n",
    "gmm_df.columns = gmm_df.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 3: Which agents are in which clusters?\n",
    "# # ################################################################################################################################################\n",
    "# # find_agent_df=pd.concat([gmm_df['client_interest_grp'],df['agntnum']],axis=1)\n",
    "\n",
    "# # print('The following displays agents and their new lable')\n",
    "# # print(find_agent_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4 Input into random forest\n",
    "# ################################################################################################################################################\n",
    "\n",
    "#Select client specific data and set cluster lable: 'client_interest_label' as Random Forest target\n",
    "print('This is the df used for random forest:')\n",
    "pre_gmm_df=gmm_df.drop(columns=['client_interest_grp'])\n",
    "print(pre_gmm_df.head())\n",
    "print(pre_gmm_df.columns)\n",
    "\n",
    "#############################\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have a target variable 'target'\n",
    "\n",
    "target = gmm_df['client_interest_grp']  # Replace 'target' with the actual target column name\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(pre_gmm_df, target, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=42,max_depth=4,\n",
    "                             n_estimators=100,min_samples_leaf=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('////////////////////////////////////////////////////////////')\n",
    "print(f'Accuracy in predicting {target}: {accuracy}')\n",
    "\n",
    "#add clustering result to d\n",
    "# d['client_interest_grp']=gmm_df['client_interest_grp'].copy()\n",
    "d.loc[:, 'client_interest_grp'] = gmm_df['client_interest_grp'].copy()\n",
    "\n",
    "\n",
    "print('//////////////////////////////////////////////')\n",
    "print(d.head())\n",
    "print(d.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Uncomment module to use pca\n",
    "################################################################################################\n",
    "\n",
    "# # Handle missing values (fill with mean for numerical columns)\n",
    "# clustering_df_numeric.fillna(clustering_df_numeric.mean(), inplace=True)\n",
    "\n",
    "# # Standardize the numeric data\n",
    "# scaler = StandardScaler()\n",
    "# df_scaled = scaler.fit_transform(clustering_df_numeric)\n",
    "\n",
    "# # Apply PCA\n",
    "# pca = PCA(n_components=0.95)  # Retain 95% variance\n",
    "# df_pca = pca.fit_transform(df_scaled)\n",
    "# df_pca = pd.DataFrame(df_pca, columns=[f'PC{i+1}' for i in range(df_pca.shape[1])])\n",
    "\n",
    "# # print(df_pca.head())\n",
    "\n",
    "# clustering_df_numeric=df_pca\n",
    "\n",
    "# clustering_df_mca = clustering_df_mca.reset_index(drop=True)\n",
    "# clustering_df_numerical = clustering_df_numeric.reset_index(drop=True)\n",
    "\n",
    "# print(clustering_df_mca.index.equals(clustering_df_numeric.index))\n",
    "# ###############################################################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4 Input into random forest prt 2\n",
    "# ################################################################################################################################################\n",
    "\n",
    "print(clustering_df_numeric.head())\n",
    "#rejoin mca and numerical for random forest predictions\n",
    "\n",
    "# print(clustering_df_mca.shape)\n",
    "# print(clustering_df_numerical.shape)\n",
    "\n",
    "gmm_df=pd.concat([clustering_df_mca,clustering_df_numeric],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#convert column names to string\n",
    "gmm_df.columns = gmm_df.columns.astype(str)\n",
    "\n",
    "\n",
    "\n",
    "## 2.2: Fit post mca data to GMM\n",
    "########################################################################\n",
    "print('This is the dataframe used for GMM clustering:')\n",
    "print(gmm_df.head)\n",
    "print(gmm_df.columns)\n",
    "\n",
    "gmm = GaussianMixture(n_components = 10)\n",
    "\n",
    "print('checkpoint 2')\n",
    "\n",
    "# Fit the GMM model for the dataset \n",
    "\n",
    "gmm.fit(gmm_df)\n",
    "\n",
    "print('checkpoint 2')\n",
    "# Assign a label to each sample\n",
    "labels = gmm.predict(gmm_df)\n",
    "\n",
    "gmm_df['client_interest_grp']= labels\n",
    "\n",
    "# print the converged log-likelihood value\n",
    "print(gmm.lower_bound_)\n",
    " \n",
    "# print the number of iterations needed\n",
    "# for the log-likelihood value to converge\n",
    "print(gmm.n_iter_)\n",
    "print('checkpoint 3')\n",
    "\n",
    "print('This is the dataframe with the new cluster lable:')\n",
    "print(gmm_df.head())\n",
    "print(gmm_df['client_interest_grp'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 5: XGBoost Rank Model for Agent Recommendation Within Clusters\n",
    "#################################################################################################################3\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure 'client_interest_grp' is assigned to df\n",
    "df = df.merge(d[['client_interest_grp']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Define ranking score (modify as needed)\n",
    "df['ranking_score'] = df['cnt_converted'] / (df['pct_inforce'] + 1e-5)  # Example: Conversion Rate\n",
    "\n",
    "# Features for ranking (agent-specific & client-agent interaction features)\n",
    "ranking_features = ['cnt_converted', 'annual_premium_cnvrt', 'pct_lapsed', \n",
    "                    'pct_cancel', 'pct_inforce', 'pct_prod_0_cnvrt', 'pct_prod_1_cnvrt', \n",
    "                    'pct_prod_2_cnvrt', 'pct_prod_3_cnvrt', 'pct_prod_4_cnvrt','pct_prod_5_cnvrt',\n",
    "                    'pct_prod_6_cnvrt', 'pct_prod_7_cnvrt', 'pct_prod_8_cnvrt', 'pct_prod_9_cnvrt']\n",
    "\n",
    "# Prepare ranking data\n",
    "X = df[ranking_features]\n",
    "y = df['ranking_score']\n",
    "group = df['client_interest_grp'].values  # One value per row\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    X, y, group, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "group_train_sizes = np.unique(group_train, return_counts=True)[1]\n",
    "group_test_sizes = np.unique(group_test, return_counts=True)[1]\n",
    "\n",
    "# Convert to DMatrix for XGBoost Rank\n",
    "train_data = xgb.DMatrix(X_train, label=y_train)\n",
    "train_data.set_group(group_train_sizes)\n",
    "test_data = xgb.DMatrix(X_test, label=y_test)\n",
    "test_data.set_group(group_test_sizes)\n",
    "print(f\"Length of X_train: {len(X_train)}, Sum of group_train_sizes: {sum(group_train_sizes)}\")\n",
    "print(f\"Length of X_test: {len(X_test)}, Sum of group_test_sizes: {sum(group_test_sizes)}\")\n",
    "\n",
    "# XGBoost Rank model parameters\n",
    "params = {\n",
    "    'objective': 'rank:pairwise',  # or 'rank:ndcg' for better performance\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'eval_metric': 'ndcg'\n",
    "}\n",
    "\n",
    "# Train XGBoost Rank model\n",
    "rank_model = xgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "# Predict agent rankings for test data\n",
    "y_pred = rank_model.predict(test_data)\n",
    "\n",
    "# Assign predicted scores to agents for ranking\n",
    "df.loc[X_test.index, 'rank_score'] = y_pred\n",
    "\n",
    "# Rank agents within each client cluster\n",
    "df['rank_within_cluster'] = df.groupby('client_interest_grp')['rank_score'].rank(ascending=False)\n",
    "\n",
    "# Select the top agent per cluster for recommendation\n",
    "top_agents = df.loc[df['rank_within_cluster'] == 1, ['client_interest_grp', 'agntnum', 'rank_score']]\n",
    "\n",
    "print(top_agents)\n",
    "print(\"Top recommended agents have been identified!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval\n",
    "###########################################################################################\n",
    "\n",
    "# Import necessary libraries for evaluation\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, accuracy_score, classification_report, ndcg_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/merged_data.csv\")\n",
    "\n",
    "# Data Preprocessing\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "# Encode agent expertise\n",
    "expertise_list = ['prod_0', 'prod_2', 'prod_4', 'prod_6', 'prod_7', 'prod_8', 'prod_9']\n",
    "for expertise in expertise_list:\n",
    "    df[f'agent_expertise_{expertise}'] = df['agent_product_expertise'].apply(lambda x: 1 if expertise in str(x) else 0)\n",
    "df.drop(columns=['agent_product_expertise'], inplace=True)\n",
    "\n",
    "# Filter relevant rows\n",
    "df = df[(df['flg_expire'] != 1) & (df['flg_lapsed'] != 1) & (df['annual_premium'] > 0)]\n",
    "\n",
    "# Define client interest columns\n",
    "client_interest_columns = [\n",
    "    'annual_premium', 'product', 'product_grp', 'agent_age', 'agent_gender', 'agent_marital', 'agent_tenure', \n",
    "    'cnt_converted', 'annual_premium_cnvrt', 'pct_lapsed', 'pct_cancel', 'pct_inforce', \n",
    "    'pct_prod_0_cnvrt', 'pct_prod_1_cnvrt', 'pct_prod_2_cnvrt', 'pct_prod_3_cnvrt', 'pct_prod_4_cnvrt', \n",
    "    'pct_prod_5_cnvrt', 'pct_prod_6_cnvrt', 'pct_prod_7_cnvrt', 'pct_prod_8_cnvrt', 'pct_prod_9_cnvrt', \n",
    "    'pct_SX0_unknown', 'pct_SX1_male', 'pct_SX2_female', 'pct_AG01_lt20', 'pct_AG02_20to24', 'pct_AG03_25to29', \n",
    "    'pct_AG04_30to34', 'pct_AG05_35to39', 'pct_AG06_40to44', 'pct_AG07_45to49', 'pct_AG08_50to54', \n",
    "    'pct_AG09_55to59', 'pct_AG10_60up', 'cluster'\n",
    "]\n",
    "\n",
    "d = df[client_interest_columns].copy()\n",
    "\n",
    "# One-hot encode categorical features\n",
    "d = pd.get_dummies(d, drop_first=True)\n",
    "d = d.astype(float)\n",
    "\n",
    "# MCA Transformation\n",
    "import prince\n",
    "mca = prince.MCA(n_components=10, random_state=42)\n",
    "mca.fit(d)\n",
    "mca_transformed = mca.transform(d)\n",
    "\n",
    "# Clustering with GMM\n",
    "gmm = GaussianMixture(n_components=10, random_state=42)\n",
    "gmm.fit(mca_transformed)\n",
    "\n",
    "# Assign cluster labels\n",
    "d['client_interest_grp'] = gmm.predict(mca_transformed)\n",
    "\n",
    "# Clustering Evaluation\n",
    "silhouette_avg = silhouette_score(mca_transformed, d['client_interest_grp'])\n",
    "davies_bouldin = davies_bouldin_score(mca_transformed, d['client_interest_grp'])\n",
    "log_likelihood = gmm.lower_bound_\n",
    "\n",
    "# Prepare data for classification\n",
    "pre_gmm_df = mca_transformed\n",
    "target = d['client_interest_grp']\n",
    "\n",
    "# Split data for classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(pre_gmm_df, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "clf = RandomForestClassifier(random_state=42, max_depth=4, n_estimators=100, min_samples_leaf=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Classification Evaluation\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Ranking Model (XGBoost Ranker)\n",
    "df['ranking_score'] = df['cnt_converted'] / (df['pct_inforce'] + 1e-5)\n",
    "\n",
    "ranking_features = ['cnt_converted', 'annual_premium_cnvrt', 'pct_lapsed', 'pct_cancel', 'pct_inforce', \n",
    "                    'pct_prod_0_cnvrt', 'pct_prod_1_cnvrt', 'pct_prod_2_cnvrt', 'pct_prod_3_cnvrt', 'pct_prod_4_cnvrt']\n",
    "\n",
    "X = df[ranking_features]\n",
    "y = df['ranking_score']\n",
    "group = d['client_interest_grp'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    X, y, group, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "group_train_sizes = np.unique(group_train, return_counts=True)[1]\n",
    "group_test_sizes = np.unique(group_test, return_counts=True)[1]\n",
    "\n",
    "# Convert to DMatrix\n",
    "train_data = xgb.DMatrix(X_train, label=y_train)\n",
    "train_data.set_group(group_train_sizes)\n",
    "test_data = xgb.DMatrix(X_test, label=y_test)\n",
    "test_data.set_group(group_test_sizes)\n",
    "\n",
    "# Train XGBoost Ranker\n",
    "params = {'objective': 'rank:pairwise', 'eta': 0.1, 'max_depth': 6, 'eval_metric': 'ndcg'}\n",
    "rank_model = xgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "# Predict rankings\n",
    "y_pred_rank = rank_model.predict(test_data)\n",
    "\n",
    "# Compute NDCG@5\n",
    "ndcg_score_5 = ndcg_score([y_test], [y_pred_rank], k=5)\n",
    "\n",
    "# Compute MRR\n",
    "reciprocal_ranks = [1 / (np.where(np.argsort(y_pred_rank)[::-1] == i)[0][0] + 1) for i in range(len(y_test))]\n",
    "mrr = np.mean(reciprocal_ranks)\n",
    "\n",
    "# Store results\n",
    "results = {\n",
    "    \"Clustering\": {\n",
    "        \"Silhouette Score\": silhouette_avg,\n",
    "        \"Davies-Bouldin Index\": davies_bouldin,\n",
    "        \"Log-Likelihood\": log_likelihood\n",
    "    },\n",
    "    \"Classification\": {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": classification_rep[\"weighted avg\"][\"precision\"],\n",
    "        \"Recall\": classification_rep[\"weighted avg\"][\"recall\"],\n",
    "        \"F1-score\": classification_rep[\"weighted avg\"][\"f1-score\"]\n",
    "    },\n",
    "    \"Ranking\": {\n",
    "        \"NDCG@5\": ndcg_score_5,\n",
    "        \"MRR\": mrr\n",
    "    }\n",
    "}\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Representation Learning-Based Agent-Client Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cosine_similarity.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "# Define file paths using `project_root`.\n",
    "# The key change is here:  project_root is NOW correct.\n",
    "df_agent = pd.read_csv(\"../data/agent_data.csv\")\n",
    "df_client = pd.read_csv(\"../data/client_data.csv\")\n",
    "df_policy = pd.read_csv(\"../data/policy_data.csv\")\n",
    "df = pd.read_csv(\"../data/encoded_data_with_id_names_1_tovector.csv\")\n",
    "\n",
    "\n",
    "# Define agent and customer features\n",
    "agent_feature_cols = [\n",
    "    'agntnum', 'pct_SX1_male', 'agent_marital_S', 'agent_marital_U', 'agent_marital_W', 'agent_marital_M', 'agent_marital_P', 'agent_marital_D',\n",
    "    'pct_AG01_lt20', 'pct_AG02_20to24', 'pct_AG03_25to29', 'pct_AG04_30to34',\n",
    "    'pct_AG05_35to39', 'pct_AG06_40to44', 'pct_AG07_45to49',\n",
    "    'pct_AG08_50to54', 'pct_AG09_55to59', 'pct_AG10_60up', \n",
    "    'pct_prod_0_cnvrt', 'pct_prod_1_cnvrt', 'pct_prod_2_cnvrt', 'pct_prod_3_cnvrt', \n",
    "    'pct_prod_4_cnvrt', 'pct_prod_5_cnvrt', 'pct_prod_6_cnvrt','pct_prod_7_cnvrt', \n",
    "    'pct_prod_8_cnvrt', 'pct_prod_9_cnvrt',\n",
    "    'agent_expertise_prod_0', 'agent_expertise_prod_2', 'agent_expertise_prod_4', 'agent_expertise_prod_6',\n",
    "    'agent_expertise_prod_7', 'agent_expertise_prod_8', 'agent_expertise_prod_9',\n",
    "    'economic_status_avg', 'household_size_avg', 'family_size_avg',\n",
    "    'net_indicator'\n",
    "]\n",
    "\n",
    "customer_feature_cols = [\n",
    "    'secuityno', 'cltsex_M', 'marryd_S', 'marryd_U', 'marryd_W', 'marryd_M', 'marryd_P', 'marryd_D',\n",
    "    'Cpct_AG01_lt20', 'Cpct_AG02_20to24', 'Cpct_AG03_25to29', 'Cpct_AG04_30to34',\n",
    "    'Cpct_AG05_35to39', 'Cpct_AG06_40to44', 'Cpct_AG07_45to49',\n",
    "    'Cpct_AG08_50to54', 'Cpct_AG09_55to59', 'Cpct_AG10_60up', \n",
    "    'product_prod_0', 'product_prod_1', 'product_prod_2', 'product_prod_3',\n",
    "    'product_prod_4', 'product_prod_5', 'product_prod_6', 'product_prod_7', \n",
    "    'product_prod_8', 'product_prod_9',\n",
    "    'product_prod_0', 'product_prod_2', 'product_prod_4', 'product_prod_6',\n",
    "    'product_prod_7', 'product_prod_8', 'product_prod_9',\n",
    "    'economic_status', 'household_size', 'family_size',\n",
    "    'inforce'\n",
    "]\n",
    "'''\n",
    "'cnt_converted', 'annual_premium_cnvrt','annual_premium‘\n",
    "'agent_tenure' 'cust_tenure_at_purchase_grp_encoded',\n",
    "'''\n",
    "\n",
    "# Extract agent and customer data\n",
    "df_agent = df[agent_feature_cols].fillna(0)\n",
    "df_agent = df_agent.drop_duplicates()\n",
    "df_customer = df[customer_feature_cols].fillna(0)\n",
    "df_customer = df_customer.drop_duplicates()\n",
    "\n",
    "\n",
    "def recommend_agents_unsupervised(secuityno, df_customer, df_agent, top_k=3):\n",
    "    # Ensure customer_id exists in df_customer\n",
    "    if secuityno not in df_customer['secuityno'].values:\n",
    "        raise ValueError(f\"Customer ID {secuityno} not found in dataset\")\n",
    "\n",
    "    # Get the customer profile\n",
    "    customer_profile = df_customer[df_customer['secuityno'] == secuityno].drop(columns=['secuityno']).values\n",
    "    \n",
    "    # Get agent profiles\n",
    "    agent_profiles = df_agent.drop(columns=['agntnum']).values\n",
    "    \n",
    "    # Compute cosine similarity between the customer and all agents\n",
    "    similarities = cosine_similarity(customer_profile, agent_profiles)[0]\n",
    "    \n",
    "    # Get indices of the most similar agents\n",
    "    sorted_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    # Get recommended agents\n",
    "    recommended_agents = df_agent.iloc[sorted_indices][['agntnum']]\n",
    "    \n",
    "    return recommended_agents\n",
    "\n",
    "# Example usage:\n",
    "recommended_unsupervised = recommend_agents_unsupervised(\n",
    "    secuityno= 'CIN:2161', \n",
    "    df_customer=df_customer, \n",
    "    df_agent=df_agent, \n",
    "    top_k=3,\n",
    ")\n",
    "print(\"\\nUnsupervised Recommended Agents:\")\n",
    "print(recommended_unsupervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cosine_similarity_embedding.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "\n",
    "# Define file paths using `base_dir` (No need for `./` before the path)\n",
    "df = pd.read_csv(\"../data/encoded_data_with_id_names_1_tovector.csv\")\n",
    "\n",
    "agent_feature_cols = [\n",
    "    'agntnum', 'pct_SX1_male', 'agent_marital_S', 'agent_marital_U', 'agent_marital_W', 'agent_marital_M', 'agent_marital_P', 'agent_marital_D',\n",
    "    'pct_AG01_lt20', 'pct_AG02_20to24', 'pct_AG03_25to29', 'pct_AG04_30to34',\n",
    "    'pct_AG05_35to39', 'pct_AG06_40to44', 'pct_AG07_45to49',\n",
    "    'pct_AG08_50to54', 'pct_AG09_55to59', 'pct_AG10_60up', \n",
    "    'economic_status_avg', 'household_size_avg', 'family_size_avg',\n",
    "    'net_indicator'\n",
    "]\n",
    "\n",
    "customer_feature_cols = [\n",
    "    'secuityno', 'cltsex_M', 'marryd_S', 'marryd_U', 'marryd_W', 'marryd_M', 'marryd_P', 'marryd_D',\n",
    "    'Cpct_AG01_lt20', 'Cpct_AG02_20to24', 'Cpct_AG03_25to29', 'Cpct_AG04_30to34',\n",
    "    'Cpct_AG05_35to39', 'Cpct_AG06_40to44', 'Cpct_AG07_45to49',\n",
    "    'Cpct_AG08_50to54', 'Cpct_AG09_55to59', 'Cpct_AG10_60up', \n",
    "    'economic_status', 'household_size', 'family_size',\n",
    "    'inforce'\n",
    "]\n",
    "\n",
    "df_agent = df[agent_feature_cols].fillna(0)\n",
    "df_agent = df_agent.drop_duplicates()\n",
    "df_customer = df[customer_feature_cols].fillna(0)\n",
    "df_customer = df_customer.drop_duplicates()\n",
    "\n",
    "class SimpleTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=8, num_layers=2):\n",
    "        super(SimpleTransformerEncoder, self).__init__()\n",
    "        # Project input features to d_model dimension\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        # Create a stack of transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)  # ✅ FIXED\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # Optionally project to final embedding space (same dimension for simplicity)\n",
    "        self.output_fc = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_dim)\n",
    "        x = self.input_fc(x)        # (batch_size, d_model)\n",
    "        # Transformer expects a sequence; here we add a sequence dimension of length 1:\n",
    "        x = x.unsqueeze(1)          # (batch_size, 1, d_model) ✅ Use dim=1 because batch_first=True\n",
    "        x = self.transformer_encoder(x)  # (batch_size, 1, d_model)\n",
    "        x = x.squeeze(1)            # (batch_size, d_model)\n",
    "        x = self.output_fc(x)       # (batch_size, d_model)\n",
    "        return x\n",
    "        \n",
    "# Example dimensions for agent and customer features:\n",
    "agent_input_dim = len(agent_feature_cols) - 1  # exclude the identifier column (e.g. 'agntnum')\n",
    "customer_input_dim = len(customer_feature_cols) - 1  # exclude the identifier column (e.g. 'secuityno')\n",
    "\n",
    "# Instantiate models\n",
    "agent_model = SimpleTransformerEncoder(input_dim=agent_input_dim, d_model=64, nhead=8, num_layers=2)\n",
    "customer_model = SimpleTransformerEncoder(input_dim=customer_input_dim, d_model=64, nhead=8, num_layers=2)\n",
    "\n",
    "# Drop the identifier columns and convert to float tensors\n",
    "agent_features = torch.tensor(df_agent.drop(columns=['agntnum']).values, dtype=torch.float32)\n",
    "customer_features = torch.tensor(df_customer.drop(columns=['secuityno']).values, dtype=torch.float32)\n",
    "\n",
    "# Get embeddings for all agents and customers\n",
    "agent_embeddings = agent_model(agent_features)      # shape: (n_agents, 64)\n",
    "customer_embeddings = customer_model(customer_features)  # shape: (n_customers, 64)\n",
    "df_customer = df_customer.reset_index(drop=True)\n",
    "# Define a function to recommend agents for a given customer based on cosine similarity in the learned embedding space.\n",
    "def recommend_agents_transformer(customer_id, df_customer, df_agent, customer_embeddings, agent_embeddings, top_k=3):\n",
    "    # Find index of the customer\n",
    "    cust_idx = df_customer[df_customer['secuityno'] == customer_id].index[0]  # Adjusted to use reset index\n",
    "    cust_embedding = customer_embeddings[cust_idx].unsqueeze(0)  # shape: (1, 64)\n",
    "    \n",
    "    # Compute cosine similarity between this customer and all agents\n",
    "    cos_sim = F.cosine_similarity(cust_embedding, agent_embeddings)  # shape: (n_agents,)\n",
    "    \n",
    "    # Get the top_k indices with highest similarity\n",
    "    top_indices = torch.topk(cos_sim, top_k).indices\n",
    "    recommended_agents = df_agent.iloc[top_indices.numpy()][['agntnum']]\n",
    "    return recommended_agents\n",
    "\n",
    "# Example usage:\n",
    "recommended_agents = recommend_agents_transformer(\n",
    "    customer_id='CIN:2818', \n",
    "    df_customer=df_customer, \n",
    "    df_agent=df_agent, \n",
    "    customer_embeddings=customer_embeddings, \n",
    "    agent_embeddings=agent_embeddings, \n",
    "    top_k=3\n",
    ")\n",
    "print(\"Transformer-based Recommended Agents:\")\n",
    "print(recommended_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cosine_similarity_embedding_1.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "\n",
    "df_client = pd.read_csv(\"../data/client_data.csv\")\n",
    "df = pd.read_csv(\"../data/encoded_data_with_id_names_1_tovector.csv\")\n",
    "\n",
    "agent_feature_cols = [\n",
    "    'agntnum', 'pct_SX1_male', 'agent_marital_S', 'agent_marital_U', 'agent_marital_W', 'agent_marital_M', 'agent_marital_P', 'agent_marital_D',\n",
    "    'pct_AG01_lt20', 'pct_AG02_20to24', 'pct_AG03_25to29', 'pct_AG04_30to34',\n",
    "    'pct_AG05_35to39', 'pct_AG06_40to44', 'pct_AG07_45to49',\n",
    "    'pct_AG08_50to54', 'pct_AG09_55to59', 'pct_AG10_60up',\n",
    "    'economic_status_avg', 'household_size_avg', 'family_size_avg',\n",
    "    'net_indicator'\n",
    "]\n",
    "\n",
    "customer_feature_cols = [\n",
    "     'cltsex_M', 'marryd_S', 'marryd_U', 'marryd_W', 'marryd_M', 'marryd_P', 'marryd_D',\n",
    "    'Cpct_AG01_lt20', 'Cpct_AG02_20to24', 'Cpct_AG03_25to29', 'Cpct_AG04_30to34',\n",
    "    'Cpct_AG05_35to39', 'Cpct_AG06_40to44', 'Cpct_AG07_45to49',\n",
    "    'Cpct_AG08_50to54', 'Cpct_AG09_55to59', 'Cpct_AG10_60up',\n",
    "    'economic_status', 'household_size', 'family_size',\n",
    "    'inforce'\n",
    "] #Removed secuityno\n",
    "\n",
    "df_agent = df[agent_feature_cols].fillna(0)\n",
    "df_agent = df_agent.drop_duplicates()\n",
    "#df_customer = df[customer_feature_cols].fillna(0)  # No longer needed\n",
    "#df_customer = df_customer.drop_duplicates()\n",
    "\n",
    "\n",
    "class SimpleTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=8, num_layers=2):\n",
    "        super(SimpleTransformerEncoder, self).__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_fc(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.output_fc(x)\n",
    "        return x\n",
    "\n",
    "agent_input_dim = len(agent_feature_cols) - 1  # exclude the identifier column (e.g. 'agntnum')\n",
    "customer_input_dim = len(customer_feature_cols)  #  Now, no identifier\n",
    "\n",
    "\n",
    "# Instantiate models\n",
    "agent_model = SimpleTransformerEncoder(input_dim=agent_input_dim, d_model=64, nhead=8, num_layers=2)\n",
    "customer_model = SimpleTransformerEncoder(input_dim=customer_input_dim, d_model=64, nhead=8, num_layers=2)\n",
    "\n",
    "# Drop the identifier columns and convert to float tensors\n",
    "agent_features = torch.tensor(df_agent.drop(columns=['agntnum']).values, dtype=torch.float32)\n",
    "\n",
    "# Get embeddings for all agents (customer embeddings will be generated on-the-fly)\n",
    "agent_embeddings = agent_model(agent_features)      # shape: (n_agents, 64)\n",
    "\n",
    "\n",
    "def calculate_age(born):\n",
    "    \"\"\"Calculates age based on birth date.\"\"\"\n",
    "    born = datetime.strptime(born, \"%Y-%m-%d\")\n",
    "    today = datetime(2025, 2, 6)  # Fixed date\n",
    "    return today.year - born.year - ((today.month, today.day) < (born.month, born.day))\n",
    "\n",
    "\n",
    "def create_customer_features(customer_data):\n",
    "    \"\"\"Creates a feature vector for a customer from raw data.\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    # cltsex\n",
    "    features['cltsex_M'] = 1 if customer_data['cltsex'] == 'M' else 0\n",
    "\n",
    "    # marryd\n",
    "    for status in ['S', 'U', 'W', 'M', 'P', 'D']:\n",
    "        features[f'marryd_{status}'] = 1 if customer_data['marryd'] == status else 0\n",
    "\n",
    "    # Age groups\n",
    "    age = calculate_age(customer_data['cltdob'])\n",
    "    age_groups = ['Cpct_AG01_lt20', 'Cpct_AG02_20to24', 'Cpct_AG03_25to29', 'Cpct_AG04_30to34',\n",
    "                  'Cpct_AG05_35to39', 'Cpct_AG06_40to44', 'Cpct_AG07_45to49',\n",
    "                  'Cpct_AG08_50to54', 'Cpct_AG09_55to59', 'Cpct_AG10_60up']\n",
    "    for group in age_groups:\n",
    "        features[group] = 0\n",
    "    if age < 20:\n",
    "        features['Cpct_AG01_lt20'] = 1\n",
    "    elif 20 <= age <= 24:\n",
    "        features['Cpct_AG02_20to24'] = 1\n",
    "    elif 25 <= age <= 29:\n",
    "      features['Cpct_AG03_25to29'] = 1\n",
    "    elif 30 <= age <= 34:\n",
    "      features['Cpct_AG04_30to34'] = 1\n",
    "    elif 35 <= age <= 39:\n",
    "        features['Cpct_AG05_35to39'] = 1\n",
    "    elif 40 <= age <= 44:\n",
    "        features['Cpct_AG06_40to44'] = 1\n",
    "    elif 45 <= age <= 49:\n",
    "      features['Cpct_AG07_45to49'] = 1\n",
    "    elif 50 <= age <= 54:\n",
    "      features['Cpct_AG08_50to54'] = 1\n",
    "    elif 55 <= age <= 59:\n",
    "        features['Cpct_AG09_55to59'] = 1\n",
    "    elif age >= 60:\n",
    "        features['Cpct_AG10_60up'] = 1\n",
    "\n",
    "    # Other features\n",
    "    features['economic_status'] = customer_data.get('economic_status', 0)  # Use .get() for safety\n",
    "    features['household_size'] = customer_data.get('household_size', 0)\n",
    "    features['family_size'] = customer_data.get('family_size', 0)\n",
    "    features['inforce'] = customer_data.get('inforce', 0)\n",
    "\n",
    "\n",
    "    # Convert to DataFrame and then to tensor\n",
    "    features_df = pd.DataFrame([features])\n",
    "    features_tensor = torch.tensor(features_df.values, dtype=torch.float32)\n",
    "    return features_tensor\n",
    "\n",
    "def recommend_agents_transformer(customer_data, df_agent, agent_embeddings, customer_model, top_k=3):\n",
    "    \"\"\"Recommends agents for a given customer based on their data.\"\"\"\n",
    "\n",
    "    # Create customer embedding\n",
    "    customer_features = create_customer_features(customer_data)\n",
    "    customer_embedding = customer_model(customer_features)  # (1, 64)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cos_sim = F.cosine_similarity(customer_embedding, agent_embeddings)  # (n_agents,)\n",
    "\n",
    "    # Get top_k indices\n",
    "    top_indices = torch.topk(cos_sim, top_k).indices\n",
    "    recommended_agents = df_agent.iloc[top_indices.numpy()][['agntnum']]\n",
    "    return recommended_agents\n",
    "\n",
    "\n",
    "\n",
    "# Example usage with a dictionary:\n",
    "customer_data = {\n",
    "    'cltsex': 'M',\n",
    "    'cltdob': '1990-05-15',\n",
    "    'marryd': 'M',\n",
    "    'economic_status': 5,\n",
    "    'household_size': 3,\n",
    "    'family_size': 2,\n",
    "    'inforce': 1  # Example value, adjust as needed\n",
    "}\n",
    "\n",
    "recommended_agents = recommend_agents_transformer(\n",
    "    customer_data=customer_data,\n",
    "    df_agent=df_agent,\n",
    "    agent_embeddings=agent_embeddings,\n",
    "    customer_model=customer_model,\n",
    "    top_k=3\n",
    ")\n",
    "print(\"Transformer-based Recommended Agents:\")\n",
    "print(recommended_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cosine_similarity_training.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# =============================================================================\n",
    "# Data Loading and Preprocessing (your provided code)\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"../data/encoded_data_with_id_names_1_tovector.csv\")\n",
    "df_filter = pd.read_csv(\"../data/positive_examples.csv\")\n",
    "\n",
    "agent_feature_cols = [\n",
    "    'agntnum', 'pct_SX1_male', 'agent_marital_S', 'agent_marital_U', 'agent_marital_W',\n",
    "    'agent_marital_M', 'agent_marital_P', 'agent_marital_D',\n",
    "    'pct_AG01_lt20', 'pct_AG02_20to24', 'pct_AG03_25to29', 'pct_AG04_30to34',\n",
    "    'pct_AG05_35to39', 'pct_AG06_40to44', 'pct_AG07_45to49',\n",
    "    'pct_AG08_50to54', 'pct_AG09_55to59', 'pct_AG10_60up', \n",
    "    'economic_status_avg', 'household_size_avg', 'family_size_avg',\n",
    "    'net_indicator'\n",
    "]\n",
    "\n",
    "customer_feature_cols = [\n",
    "    'secuityno', 'cltsex_M', 'marryd_S', 'marryd_U', 'marryd_W', 'marryd_M',\n",
    "    'marryd_P', 'marryd_D',\n",
    "    'Cpct_AG01_lt20', 'Cpct_AG02_20to24', 'Cpct_AG03_25to29', 'Cpct_AG04_30to34',\n",
    "    'Cpct_AG05_35to39', 'Cpct_AG06_40to44', 'Cpct_AG07_45to49',\n",
    "    'Cpct_AG08_50to54', 'Cpct_AG09_55to59', 'Cpct_AG10_60up', \n",
    "    'economic_status', 'household_size', 'family_size',\n",
    "    'inforce'\n",
    "]\n",
    "\n",
    "# Fill missing values and remove duplicates\n",
    "df_agent = df[agent_feature_cols].fillna(0).drop_duplicates()\n",
    "df_customer = df[customer_feature_cols].fillna(0).drop_duplicates()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Transformer Model Definition (your provided code with batch_first fixed)\n",
    "# =============================================================================\n",
    "class SimpleTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=8, num_layers=2):\n",
    "        super(SimpleTransformerEncoder, self).__init__()\n",
    "        # Project input features to d_model dimension\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        # Create a stack of transformer encoder layers; using batch_first=True for convenience\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # Project to final embedding space\n",
    "        self.output_fc = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_dim)\n",
    "        x = self.input_fc(x)        # (batch_size, d_model)\n",
    "        x = x.unsqueeze(1)          # (batch_size, 1, d_model)\n",
    "        x = self.transformer_encoder(x)  # (batch_size, 1, d_model)\n",
    "        x = x.squeeze(1)            # (batch_size, d_model)\n",
    "        x = self.output_fc(x)       # (batch_size, d_model)\n",
    "        return x\n",
    "\n",
    "# Set dimensions (exclude identifier columns)\n",
    "agent_input_dim = len(agent_feature_cols) - 1  # excluding 'agntnum'\n",
    "customer_input_dim = len(customer_feature_cols) - 1  # excluding 'secuityno'\n",
    "\n",
    "# Instantiate models\n",
    "agent_model = SimpleTransformerEncoder(input_dim=agent_input_dim, d_model=64, nhead=8, num_layers=2)\n",
    "customer_model = SimpleTransformerEncoder(input_dim=customer_input_dim, d_model=64, nhead=8, num_layers=2)\n",
    "\n",
    "# =============================================================================\n",
    "# Preparing Dummy Conversion Data for Training\n",
    "# =============================================================================\n",
    "# In practice, you should use your historical conversion data (positive pairs).\n",
    "# Here, we simulate a training batch by randomly sampling pairs from the customer and agent data.\n",
    "# Note: This is for demonstration only.\n",
    "\n",
    "df_filter = df_filter.sort_values(by='cosine_sim', ascending=False).reset_index(drop=True)\n",
    "cutoff_index = int(0.3 * len(df_filter))  # 20% of total rows\n",
    "df_top_20pct = df_filter.iloc[:cutoff_index].copy()\n",
    "\n",
    "# Extract just the agent columns from the top-20% subset\n",
    "train_agent_df = df_top_20pct[agent_feature_cols].copy()\n",
    "\n",
    "# Extract just the customer columns from the top-20% subset\n",
    "train_customer_df = df_top_20pct[customer_feature_cols].copy()\n",
    "\n",
    "# Convert the features (drop identifier columns) to tensors\n",
    "train_customer_tensor = torch.tensor(train_customer_df.drop(columns=['secuityno']).values, dtype=torch.float32)\n",
    "train_agent_tensor = torch.tensor(train_agent_df.drop(columns=['agntnum']).values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Contrastive Loss (InfoNCE) Definition\n",
    "# =============================================================================\n",
    "def contrastive_loss(customer_embeddings, agent_embeddings, temperature=0.07):\n",
    "    \"\"\"\n",
    "    Computes the InfoNCE loss between customer and agent embeddings.\n",
    "    Assumes that the i-th customer in the batch is paired with the i-th agent as the positive pair.\n",
    "    All other pairs in the batch are treated as negatives.\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity matrix between customer and agent embeddings: shape (N, N)\n",
    "    similarity_matrix = F.cosine_similarity(customer_embeddings.unsqueeze(1), agent_embeddings.unsqueeze(0), dim=-1)\n",
    "    # Scale the similarity scores by the temperature\n",
    "    similarity_matrix = similarity_matrix / temperature\n",
    "    \n",
    "    # For each customer, the correct (positive) agent is at the diagonal (i.e., same index).\n",
    "    targets = torch.arange(similarity_matrix.size(0)).to(similarity_matrix.device)\n",
    "    \n",
    "    # Compute cross-entropy loss in both directions for symmetry:\n",
    "    loss_cust_to_agent = F.cross_entropy(similarity_matrix, targets)\n",
    "    loss_agent_to_cust = F.cross_entropy(similarity_matrix.T, targets)\n",
    "    \n",
    "    loss = (loss_cust_to_agent + loss_agent_to_cust) / 2\n",
    "    return loss\n",
    "\n",
    "# =============================================================================\n",
    "# Training and Fine-Tuning Loop\n",
    "# =============================================================================\n",
    "temperature = 0.07\n",
    "optimizer = optim.Adam(list(agent_model.parameters()) + list(customer_model.parameters()), lr=1e-3)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In a real training scenario, you would iterate over a DataLoader with many batches.\n",
    "    # Here, we use our dummy batch.\n",
    "    agent_model.train()\n",
    "    customer_model.train()\n",
    "    \n",
    "    # Compute embeddings for the current batch\n",
    "    customer_embeds = customer_model(train_customer_tensor)  # shape: (batch_size, d_model)\n",
    "    agent_embeds = agent_model(train_agent_tensor)          # shape: (batch_size, d_model)\n",
    "    \n",
    "    # Compute the contrastive loss\n",
    "    loss = contrastive_loss(customer_embeds, agent_embeds, temperature)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'agent_model_state_dict': agent_model.state_dict(),\n",
    "    'customer_model_state_dict': customer_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, 'checkpoint.pth')\n",
    "\n",
    "# =============================================================================\n",
    "# After Training: Use the Fine-Tuned Models for Recommendation\n",
    "# =============================================================================\n",
    "# Get embeddings for all agents and customers using the fine-tuned models.\n",
    "# (Convert the corresponding DataFrames to tensors as before.)\n",
    "\n",
    "agent_features = torch.tensor(df_agent.drop(columns=['agntnum']).values, dtype=torch.float32)\n",
    "customer_features = torch.tensor(df_customer.drop(columns=['secuityno']).values, dtype=torch.float32)\n",
    "\n",
    "agent_embeddings = agent_model(agent_features)      # shape: (n_agents, d_model)\n",
    "customer_embeddings = customer_model(customer_features)  # shape: (n_customers, d_model)\n",
    "\n",
    "# Define the recommendation function using the fine-tuned embeddings.\n",
    "def recommend_agents_transformer(customer_id, df_customer, df_agent, customer_embeddings, agent_embeddings, top_k=3):\n",
    "    # Find the index of the given customer in df_customer.\n",
    "    cust_idx = df_customer.index[df_customer['secuityno'] == customer_id].tolist()[0]\n",
    "    cust_embedding = customer_embeddings[cust_idx].unsqueeze(0)  # shape: (1, d_model)\n",
    "    \n",
    "    # Compute cosine similarity between this customer's embedding and all agent embeddings.\n",
    "    cos_sim = F.cosine_similarity(cust_embedding, agent_embeddings)  # shape: (n_agents,)\n",
    "    \n",
    "    # Get the top_k indices with the highest similarity scores.\n",
    "    top_indices = torch.topk(cos_sim, top_k).indices\n",
    "    recommended_agents = df_agent.iloc[top_indices.numpy()][['agntnum']]\n",
    "    return recommended_agents\n",
    "\n",
    "# Example usage of the fine-tuned model:\n",
    "recommended_agents = recommend_agents_transformer(\n",
    "    customer_id='CIN:2161', \n",
    "    df_customer=df_customer, \n",
    "    df_agent=df_agent, \n",
    "    customer_embeddings=customer_embeddings, \n",
    "    agent_embeddings=agent_embeddings, \n",
    "    top_k=3\n",
    ")\n",
    "print(\"Transformer-based Recommended Agents (Fine-Tuned):\")\n",
    "print(recommended_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# 1. Load data\n",
    "\n",
    "df = pd.read_csv(\"../data/encoded_data_with_id_names_1_tovector.csv\")\n",
    "\n",
    "agent_feature_cols = [\n",
    "    'agntnum', 'pct_SX1_male', 'agent_marital_S', 'agent_marital_U', 'agent_marital_W',\n",
    "    'agent_marital_M', 'agent_marital_P', 'agent_marital_D',\n",
    "    'pct_AG01_lt20', 'pct_AG02_20to24', 'pct_AG03_25to29', 'pct_AG04_30to34',\n",
    "    'pct_AG05_35to39', 'pct_AG06_40to44', 'pct_AG07_45to49',\n",
    "    'pct_AG08_50to54', 'pct_AG09_55to59', 'pct_AG10_60up',\n",
    "    'economic_status_avg', 'household_size_avg', 'family_size_avg',\n",
    "    'net_indicator'\n",
    "]\n",
    "\n",
    "customer_feature_cols = [\n",
    "    'secuityno', 'cltsex_M', 'marryd_S', 'marryd_U', 'marryd_W', 'marryd_M',\n",
    "    'marryd_P', 'marryd_D',\n",
    "    'Cpct_AG01_lt20', 'Cpct_AG02_20to24', 'Cpct_AG03_25to29', 'Cpct_AG04_30to34',\n",
    "    'Cpct_AG05_35to39', 'Cpct_AG06_40to44', 'Cpct_AG07_45to49',\n",
    "    'Cpct_AG08_50to54', 'Cpct_AG09_55to59', 'Cpct_AG10_60up',\n",
    "    'economic_status', 'household_size', 'family_size',\n",
    "    'inforce'\n",
    "]\n",
    "\n",
    "# --- CRITICAL FIX: Reset index after dropping duplicates ---\n",
    "df_agent = df[agent_feature_cols].fillna(0).drop_duplicates().reset_index(drop=True)\n",
    "df_customer = df[customer_feature_cols].fillna(0).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 2. Define model architecture (Same as before)\n",
    "class SimpleTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=8, num_layers=2):\n",
    "        super(SimpleTransformerEncoder, self).__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_fc(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.output_fc(x)\n",
    "        return x\n",
    "\n",
    "agent_input_dim = len(agent_feature_cols) - 1\n",
    "customer_input_dim = len(customer_feature_cols) - 1\n",
    "\n",
    "agent_model = SimpleTransformerEncoder(agent_input_dim, 64, 8, 2)\n",
    "customer_model = SimpleTransformerEncoder(customer_input_dim, 64, 8, 2)\n",
    "\n",
    "# 3. Load checkpoint (Same as before)\n",
    "checkpoint = torch.load(\"../src/checkpoint.pth\")\n",
    "agent_model.load_state_dict(checkpoint['agent_model_state_dict'])\n",
    "customer_model.load_state_dict(checkpoint['customer_model_state_dict'])\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']}\")\n",
    "\n",
    "# 4. Create embeddings (Same as before)\n",
    "agent_features = torch.tensor(df_agent.drop(columns=['agntnum']).values, dtype=torch.float32)\n",
    "customer_features = torch.tensor(df_customer.drop(columns=['secuityno']).values, dtype=torch.float32)\n",
    "agent_model.eval()\n",
    "customer_model.eval()\n",
    "with torch.no_grad():\n",
    "    agent_embeddings = agent_model(agent_features)\n",
    "    customer_embeddings = customer_model(customer_features)\n",
    "\n",
    "# 5. Recommendation function (Same as before)\n",
    "def recommend_agents_transformer(customer_id, df_customer, df_agent, customer_embeddings, agent_embeddings, top_k=3):\n",
    "    cust_idx_list = df_customer.index[df_customer['secuityno'] == customer_id].tolist()\n",
    "    if not cust_idx_list:\n",
    "        print(f\"Warning: Customer {customer_id} not found in df_customer.\")\n",
    "        return None\n",
    "    cust_idx = cust_idx_list[0]\n",
    "    if cust_idx >= customer_embeddings.shape[0]:\n",
    "        print(f\"Error: Index {cust_idx} is out of bounds for customer_embeddings with size {customer_embeddings.shape[0]}\")\n",
    "        return None\n",
    "    cust_embed = customer_embeddings[cust_idx].unsqueeze(0)\n",
    "    cos_sim = F.cosine_similarity(cust_embed, agent_embeddings)\n",
    "    top_idx = torch.topk(cos_sim, top_k).indices\n",
    "    return df_agent.iloc[top_idx.numpy()][['agntnum']]\n",
    "\n",
    "# 6. Evaluate with a simple NDCG (Same as before)\n",
    "def ndcg_at_k(recommended_agents, ground_truth_agents, k=3):\n",
    "    dcg = 0.0\n",
    "    for i, agent in enumerate(recommended_agents[:k], start=1):\n",
    "        rel = 1 if agent in ground_truth_agents else 0\n",
    "        dcg += rel / np.log2(i + 1)\n",
    "    ideal_count = min(len(ground_truth_agents), k)\n",
    "    idcg = sum(1 / np.log2(i + 1) for i in range(1, ideal_count + 1))\n",
    "    return 0 if idcg == 0 else dcg / idcg\n",
    "\n",
    "# 7. compare_recommendations_with_ground_truth (Corrected Version)\n",
    "def compare_recommendations_with_ground_truth(\n",
    "    customer_id,\n",
    "    recommended_agent_ids,\n",
    "    ground_truth_agent_ids,\n",
    "    df_customer,\n",
    "    df_agent,\n",
    "    customer_embeddings,\n",
    "    agent_embeddings\n",
    "):\n",
    "    cust_idx_list = df_customer.index[df_customer['secuityno'] == customer_id].tolist()\n",
    "    if not cust_idx_list:\n",
    "        return {\"error\": f\"Customer {customer_id} not found in df_customer\"}\n",
    "    cust_idx = cust_idx_list[0]\n",
    "    customer_embed = customer_embeddings[cust_idx].unsqueeze(0)\n",
    "\n",
    "    recommended_sims = []\n",
    "    for agent_id in recommended_agent_ids:\n",
    "        agent_idx_list = df_agent.index[df_agent['agntnum'] == agent_id].tolist()\n",
    "        if not agent_idx_list:\n",
    "            continue\n",
    "        agent_idx = agent_idx_list[0]\n",
    "        agent_embed = agent_embeddings[agent_idx].unsqueeze(0)\n",
    "        sim_val = F.cosine_similarity(customer_embed, agent_embed).item()\n",
    "        recommended_sims.append((agent_id, sim_val))\n",
    "\n",
    "    ground_truth_sims = []\n",
    "    for agent_id in ground_truth_agent_ids:\n",
    "        agent_idx_list = df_agent.index[df_agent['agntnum'] == agent_id].tolist()\n",
    "        if not agent_idx_list:\n",
    "            continue\n",
    "        agent_idx = agent_idx_list[0]\n",
    "        agent_embed = agent_embeddings[agent_idx].unsqueeze(0)\n",
    "        sim_val = F.cosine_similarity(customer_embed, agent_embed).item()\n",
    "        ground_truth_sims.append((agent_id, sim_val))\n",
    "\n",
    "    if recommended_sims:\n",
    "        max_recommended_sim = max(s for _, s in recommended_sims)\n",
    "    else:\n",
    "        max_recommended_sim = 0.0\n",
    "\n",
    "    if ground_truth_sims:\n",
    "        max_ground_truth_sim = max(s for _, s in ground_truth_sims)\n",
    "    else:\n",
    "        max_ground_truth_sim = 0.0\n",
    "\n",
    "    is_better = (max_recommended_sim > max_ground_truth_sim)\n",
    "    matched_any_gt = any(agent_id in ground_truth_agent_ids for agent_id, _ in recommended_sims)\n",
    "\n",
    "    return {\n",
    "        \"recommended_sims\": recommended_sims,\n",
    "        \"ground_truth_sims\": ground_truth_sims,\n",
    "        \"max_recommended_sim\": max_recommended_sim,\n",
    "        \"max_ground_truth_sim\": max_ground_truth_sim,\n",
    "        \"is_better_than_gt\": is_better,\n",
    "        \"matched_any_gt\": matched_any_gt\n",
    "    }\n",
    "#8 Evaluation Loop\n",
    "df_filter = pd.read_csv(\"../data/positive_examples.csv\")\n",
    "df_filter = df_filter.sort_values(by='cosine_sim', ascending=False).reset_index(drop=True)\n",
    "total_rows = len(df_filter)\n",
    "start_index = int(0.3 * total_rows)\n",
    "end_index = int(0.4 * total_rows)\n",
    "df_top_30_40pct = df_filter.iloc[start_index:end_index].copy()\n",
    "df_top_30_40pct['agntnum'] = df_top_30_40pct['agntnum'].str.extract(r'(\\d+)').astype(int)\n",
    "ground_truth_mapping = df_top_30_40pct.groupby('secuityno')['agntnum'].apply(lambda x: set(x)).to_dict()\n",
    "test_customers = list(ground_truth_mapping.keys())\n",
    "ndcg_scores = []\n",
    "k = 3\n",
    "\n",
    "for cust_id in test_customers:\n",
    "    recommended_df = recommend_agents_transformer(\n",
    "        cust_id, df_customer, df_agent, customer_embeddings, agent_embeddings, top_k=k\n",
    "    )\n",
    "    if recommended_df is None:\n",
    "        continue\n",
    "    recommended_agents = recommended_df['agntnum'].tolist()\n",
    "    ground_truth_agents = ground_truth_mapping[cust_id]\n",
    "    ndcg_score = ndcg_at_k(recommended_agents, ground_truth_agents, k)\n",
    "    ndcg_scores.append(ndcg_score)\n",
    "    cmp_result = compare_recommendations_with_ground_truth(\n",
    "        customer_id=cust_id,\n",
    "        recommended_agent_ids=recommended_agents,\n",
    "        ground_truth_agent_ids=ground_truth_agents,\n",
    "        df_customer=df_customer,\n",
    "        df_agent=df_agent,\n",
    "        customer_embeddings=customer_embeddings,\n",
    "        agent_embeddings=agent_embeddings\n",
    "    )\n",
    "    print(f\"Customer {cust_id} -> recommended: {recommended_agents}, ground_truth: {ground_truth_agents}\")\n",
    "    print(f\"  NDCG@{k} = {ndcg_score:.3f}\")\n",
    "    if \"error\" not in cmp_result:\n",
    "        print(f\"  Max recommended similarity: {cmp_result['max_recommended_sim']:.4f}\")\n",
    "        print(f\"  Max ground truth similarity: {cmp_result['max_ground_truth_sim']:.4f}\")\n",
    "        print(f\"  Is recommended better than ground truth? {cmp_result['is_better_than_gt']}\")\n",
    "        print(f\"  Overlap with GT agents? {cmp_result['matched_any_gt']}\")\n",
    "    else:\n",
    "        print(f\"  Error: {cmp_result['error']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "if ndcg_scores:\n",
    "    mean_ndcg = np.mean(ndcg_scores)\n",
    "    print(f\"\\nMean NDCG@{k}: {mean_ndcg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
